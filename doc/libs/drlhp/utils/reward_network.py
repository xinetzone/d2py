import numpy as np
import torch
import torch.nn as nn

from .model.network_utils import np2torch

class RewardNetwork(nn.Module):
    """
    Class for implementing Reward network, which is used by the CustomReward wrapper to replace env reward with reward generated by learned network.
    """

    def __init__(self, env):
        super().__init__()
        self.env = env
        self.__initialize_network()

    def __initialize_network(self):
        self.lr = 3e-2
        observation_dim = self.env.observation_space.shape[0]
        action_dim = self.env.action_space.shape[0] # TODO this can only handle continous action space for now
        input_dim = observation_dim + action_dim
        self.network = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.LeakyReLU(negative_slope=0.01),
            nn.Linear(64, 64),
            nn.LeakyReLU(negative_slope=0.01),
            nn.Linear(64, 1),
            # nn.Tanh() # this makes rewards predominantly -1 or 1
            nn.BatchNorm1d(1, affine=False),
        )
        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.lr)


    def predict_reward(self, reward_input, inference = False):
        # squeeze the result so it's either [batch size] * [traj_length] or just for torch.Size([]) i.e. a scalar reward for a single (obs, action) pair
        if inference:
            with torch.no_grad():
                self.network.eval()
                reward_input = torch.unsqueeze(reward_input, 0)
                output = self.network(reward_input).squeeze()
                self.network.train()
        else:
            output = self.network(reward_input).squeeze() 
        return output
        

    def update_network(self, traj1, traj2, preferences):
        """
        actually, traj should be stored as list and then convert to np.array
        """

        """
        Args:
            traj1, traj2: a dictionary with two items, a batch of observations and actions, each an np.array of shape
             [batch size, traj_length, obs_dim or act_dim]
            preferences: np.array of shape [batch size]
            preference = 1 if traj1 is more preferable, = 2 if traj2 is more preferable, = 3 if equally preferable

        training data looks like:
        traj_1 & traj_2 (dictionary that has key "observation" and "action"),, preferences (np array of shape [batch size])

        e.g.

        # batch size = 3, traj_length = 2, observation_dim = 3, action_dim = 1
        test_traj1 = {"observations": np.array([[[1, 2, 3], [4, 5, 6]],
                                                [[1, 2, 3], [4, 5, 6]],
                                                [[1, 2, 3], [4, 5, 6]]],
                                                dtype=float), "actions": np.array([[[1],[1]],[[1],[1]],[[1],[1]]], dtype=float)}
        test_traj2 = {"observations": np.array([[[0, 2, 3], [4, 5, 6]],
                                                [[0, 2, 3], [4, 5, 6]],
                                                [[0, 2, 3], [4, 5, 6]]],
                                                dtype=float), "actions": np.array([[[1],[1]],[[1],[1]],[[1],[1]]], dtype=float)}
        test_preferences = np.array([3, 1, 2])
        
        P(s1 > s2) = exp(R(s1)) / (exp(R(s1)) + exp(R(s2))) 
        so log P(s1 > s2) = R(s1) - log(exp(R(s1)) + exp(R(s2)))

        for a numerically stable implementation, we subtract max(R(s1), R(s2)) from both R(s1) and R(s2) before computing the exponential
        this does not change the ordering
        see proof at https://jaykmody.com/blog/stable-softmax/
        
        """

        traj1_reward = torch.sum(self.traj_to_reward(traj1), dim = -1)
        traj2_reward = torch.sum(self.traj_to_reward(traj2), dim = -1)
        traj_rewards = torch.stack((traj1_reward, traj2_reward), dim=-1)

        mu1, mu2 = self.__mu(preferences)
        actual_mu = torch.stack((mu1, mu2), dim=-1)

        loss_fn = nn.CrossEntropyLoss()
        loss = loss_fn(traj_rewards, actual_mu)

        accuracy = torch.sum(torch.eq(torch.argmax(traj_rewards, dim = -1), torch.argmax(actual_mu, dim = -1))).item() / len(preferences)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
    
    def __mu(self, preferences): # preferences input dim = [batch size]
        preferences = np.asarray(preferences, dtype=np.float32)
        mu1 = np.piecewise(preferences, [preferences == 1, preferences == 2, preferences == 3], [1, 0, 1/2])
        mu2 = np.piecewise(preferences, [preferences == 1, preferences == 2, preferences == 3], [0, 1, 1/2])
        return np2torch(mu1), np2torch(mu2)
    
    def traj_to_reward(self, traj, mode = "train"):
        observations = traj["observations"]
        observations = np.asarray(observations, dtype=np.float32)
        actions = np.asarray(traj["actions"], dtype=np.float32)
        reward_input = np2torch(np.concatenate((observations, actions), axis = -1)) # now dim = batch size x traj length x (obs dim + act dim)
        batch_size, traj_length, input_dim = reward_input.shape
        reward_input = reward_input.view(-1, reward_input.shape[-1]) # now dim = (batch size * traj length) x (obs dim + act dim)
        if mode == "train":
            traj_reward = self.predict_reward(reward_input) # shape = [batch size, traj length]
        elif mode == "eval":
            traj_reward = self.predict_reward(reward_input) # possibly better to change to inference = True, currently that causes bug
        traj_reward = traj_reward.view(batch_size, traj_length) # now dim = batch size x traj length
        # make sure has size [batch size] even when batch size = 1
        if traj_reward.dim() == 1:
            traj_reward = torch.unsqueeze(traj_reward, dim = 0)
            assert traj_reward.dim() == 2
        return traj_reward