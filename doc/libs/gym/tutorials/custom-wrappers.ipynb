{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 实现自定义包装器\n",
        "\n",
        "在本教程中，将描述如何实现自定义包装器。包装器以模块化方式向环境添加功能。这将为您节省大量样板代码。\n",
        "\n",
        "我们将展示如何通过以下方式创建包装器：\n",
        "\n",
        "- 继承自 {class}`gymnasium.ObservationWrapper`\n",
        "- 继承自 {class}`gymnasium.ActionWrapper`\n",
        "- 继承自 {class}`gymnasium.RewardWrapper`\n",
        "- 继承自 {class}`gymnasium.Wrapper`\n",
        "\n",
        "在遵循此教程之前，请确保查阅 {mod}`gymnasium.wrappers` 模块的文档。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 继承自 {class}`gymnasium.ObservationWrapper`\n",
        "\n",
        "观测包装器对环境返回的观测应用某种函数时非常有用。如果您实现了观测包装器，只需通过实现 {meth}`gymnasium.ObservationWrapper.observation` 方法来定义这种转换即可。此外，如果转换改变了观测的形状（例如，通过将字典转换为 `numpy` 数组，如下例所示），您应该记得更新观测空间。\n",
        "\n",
        "想象一下您有 2D 导航任务，其中环境以包含键 ``\"agent_position\"`` 和 ``\"target_position\"`` 的字典作为观测返回。常见的做法可能是丢弃一些自由度，只考虑目标相对于智能体的位置，即 ``observation[\"target_position\"] - observation[\"agent_position\"]``。为此，您可以像这样实现观测包装器："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gymnasium import ActionWrapper, ObservationWrapper, RewardWrapper, Wrapper\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Box, Discrete\n",
        "\n",
        "\n",
        "class RelativePosition(ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.observation_space = Box(shape=(2,), low=-np.inf, high=np.inf)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return obs[\"target\"] - obs[\"agent\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 继承自 {class}`gymnasium.ActionWrapper`\n",
        "动作包装器可以用来在将动作应用到环境之前对动作进行转换。\n",
        "如果您实现了一个动作包装器，您需要通过实现 {meth}`gymnasium.ActionWrapper.action` 来定义该转换。此外，您应该通过更新包装器的动作空间来指定该转换的域。\n",
        "\n",
        "假设您有一个动作空间为 {class}`gymnasium.spaces.Box` 类型的环境，但您只想使用有限的动作子集。那么，您可能想实现以下包装器："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discrete(4)\n"
          ]
        }
      ],
      "source": [
        "class DiscreteActions(ActionWrapper):\n",
        "    def __init__(self, env, disc_to_cont):\n",
        "        super().__init__(env)\n",
        "        self.disc_to_cont = disc_to_cont\n",
        "        self.action_space = Discrete(len(disc_to_cont))\n",
        "\n",
        "    def action(self, act):\n",
        "        return self.disc_to_cont[act]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(\"LunarLanderContinuous-v3\")\n",
        "    wrapped_env = DiscreteActions(\n",
        "        env, [np.array([1, 0]), np.array([-1, 0]), np.array([0, 1]), np.array([0, -1])]\n",
        "    )\n",
        "    print(wrapped_env.action_space)  # Discrete(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 继承自 {class}`gymnasium.RewardWrapper`\n",
        "\n",
        "奖励包装器用于转换环境返回的奖励。\n",
        "与之前的包装器一样，您需要通过实现 {meth}`gymnasium.RewardWrapper.reward` 方法来指定该转换。\n",
        "\n",
        "让我们看一个例子：有时（特别是在我们无法控制奖励因为它是内在的），我们希望将奖励裁剪到一定范围内以获得一些数值稳定性。为此，我们可以实现如下包装器："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import SupportsFloat\n",
        "\n",
        "\n",
        "class ClipReward(RewardWrapper):\n",
        "    def __init__(self, env, min_reward, max_reward):\n",
        "        super().__init__(env)\n",
        "        self.min_reward = min_reward\n",
        "        self.max_reward = max_reward\n",
        "\n",
        "    def reward(self, r: SupportsFloat) -> SupportsFloat:\n",
        "        return np.clip(r, self.min_reward, self.max_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 继承自 {class}`gymnasium.Wrapper`\n",
        "有时您可能需要实现一个执行更复杂修改的包装器（例如，根据``info``中的数据修改奖励或更改渲染行为）。\n",
        "这样的包装器可以通过继承自 {class}`gymnasium.Wrapper` 来实现。\n",
        "\n",
        "- 通过在 ``__init__`` 中分别定义 ``self.action_space`` 或 ``self.observation_space`` 来设置新的动作或观察空间\n",
        "- 通过在 ``__init__`` 中定义 ``self.metadata`` 来设置新的元数据\n",
        "- 可以重写 {meth}`gymnasium.Wrapper.step`, {meth}`gymnasium.Wrapper.render`, {meth}`gymnasium.Wrapper.close` 等方法\n",
        "\n",
        "如果您这样做，可以通过访问属性 {attr}`env` 来访问传递给您的包装器的原始环境（该环境可能仍然被其他包装器包装）。\n",
        "\n",
        "让我们也看一下这种情况的一个例子。大多数MuJoCo环境返回的奖励包含不同的项：例如，可能有一项奖励智能体完成任务，另一项惩罚大动作（即能量使用）。通常，您可以在环境初始化期间传递这些项的权重参数。然而，*Reacher* 不允许这样做！不过，所有单独的奖励项都返回在 `info` 中，所以让我们为 Reacher 构建一个包装器，允许我们对这些项进行加权："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ReacherRewardWrapper(Wrapper):\n",
        "    def __init__(self, env, reward_dist_weight, reward_ctrl_weight):\n",
        "        super().__init__(env)\n",
        "        self.reward_dist_weight = reward_dist_weight\n",
        "        self.reward_ctrl_weight = reward_ctrl_weight\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, _, terminated, truncated, info = self.env.step(action)\n",
        "        reward = (\n",
        "            self.reward_dist_weight * info[\"reward_dist\"]\n",
        "            + self.reward_ctrl_weight * info[\"reward_ctrl\"]\n",
        "        )\n",
        "        return obs, reward, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "xxx",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
