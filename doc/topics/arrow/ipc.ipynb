{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 流式传输、序列化和 IPC\n",
    "\n",
    "## 写入和读取流\n",
    "\n",
    "Arrow 定义了两种二进制格式来序列化记录批次：\n",
    "\n",
    "- 流式格式：用于发送任意长度的记录批次序列。该格式必须从头到尾处理，不支持随机访问\n",
    "- 文件或随机访问格式：用于序列化固定数量的记录批次。支持随机访问，因此与内存映射一起使用时非常有用\n",
    "\n",
    "要遵循本节内容，请确保首先阅读有关[内存和 IO 的部分](https://arrow.apache.org/docs/python/memory.html#io)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用流\n",
    "首先，让我们创建一个小型记录批次："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "data = [\n",
    "    pa.array([1, 2, 3, 4]),\n",
    "    pa.array(['foo', 'bar', 'baz', None]),\n",
    "    pa.array([True, None, False, True])\n",
    "]\n",
    "\n",
    "\n",
    "batch = pa.record_batch(data, names=['f0', 'f1', 'f2'])\n",
    "\n",
    "batch.num_rows, batch.num_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以开始写入包含这些批次中一些数量的流。为此，我们使用 {class}`pyarrow.RecordBatchStreamWriter`，它可以写入可写的 {class}`pyarrow.NativeFile` 对象或可写的 Python 对象。为了方便起见，可以使用 {func}`pyarrow.ipc.new_stream` 创建这个对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sink = pa.BufferOutputStream()\n",
    "\n",
    "with pa.ipc.new_stream(sink, batch.schema) as writer:\n",
    "   for i in range(5):\n",
    "      writer.write_batch(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们使用了内存中的 Arrow 缓冲流（`sink`），但这也可以是套接字或其他 IO 接收器。\n",
    "\n",
    "创建 `StreamWriter` 时，我们传递了 `schema`，因为 `schema` （列名和类型）必须与此特定流中发送的所有批次相同。现在我们可以执行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1984"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buf = sink.getvalue()\n",
    "\n",
    "buf.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在 `buf` 包含了作为内存字节缓冲区的完整流。我们可以使用 `RecordBatchStreamReader` 或方便的函数 {func}`pyarrow.ipc.open_stream` 来读取这样的流："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f0: int64\n",
       "f1: string\n",
       "f2: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with pa.ipc.open_stream(buf) as reader:\n",
    "      schema = reader.schema\n",
    "      batches = [b for b in reader]\n",
    "\n",
    "\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以检查返回的批次是否与原始输入相同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[0].equals(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个重要的点是，如果输入源支持零拷贝读取（例如内存映射或 {class}`pyarrow.BufferReader`），则返回的批次也是零拷贝的，并且在读取时不会分配任何新内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 写入和读取随机访问文件\n",
    "`RecordBatchFileWriter` 具有与 `RecordBatchStreamWriter` 相同的API。您可以使用 {func}`~pyarrow.ipc.new_file` 创建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4226"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sink = pa.BufferOutputStream()\n",
    "\n",
    "with pa.ipc.new_file(sink, batch.schema) as writer:\n",
    "   for i in range(10):\n",
    "      writer.write_batch(batch)\n",
    "\n",
    "\n",
    "buf = sink.getvalue()\n",
    "\n",
    "buf.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RecordBatchFileReader` 和 `RecordBatchStreamReader` 之间的区别在于，输入源必须具有用于随机访问的 `seek` 方法。流读取器只需要读取操作。我们还可以使用 {func}`pyarrow.ipc.open_file` 方法打开文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pa.ipc.open_file(buf) as reader:\n",
    "   num_record_batches = reader.num_record_batches\n",
    "\n",
    "\n",
    "b = reader.get_batch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为我们能够访问整个有效负载，我们知道文件中的记录批次数量，并且可以随机读取任何一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_record_batches, b.equals(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从流和文件格式读取到 `pandas`\n",
    "流和文件读取器类具有特殊的 `read_pandas` 方法，用于简化读取多个记录批次并将它们转换为单个 `DataFrame` 输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>foo</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>bar</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>baz</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>foo</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0    f1     f2\n",
       "0   1   foo   True\n",
       "1   2   bar   None\n",
       "2   3   baz  False\n",
       "3   4  None   True\n",
       "4   1   foo   True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with pa.ipc.open_file(buf) as reader:\n",
    "   df = reader.read_pandas()\n",
    "\n",
    "\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高效地写入和读取 Arrow 数据\n",
    "\n",
    "作为针对零拷贝和内存映射数据优化的，Arrow 允许轻松读取和写入数组，同时消耗最少的驻留内存。\n",
    "\n",
    "在写入和读取原始 Arrow 数据时，我们可以使用 Arrow 文件格式或 Arrow 流式格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要将数组转储到文件，可以使用 {func}`~pyarrow.ipc.new_file`，它将提供新的 {class}`~pyarrow.ipc.RecordBatchFileWriter` 实例，可用于将数据批次写入该文件。\n",
    "\n",
    "例如，要写包含 1000 万个整数的数组，我们可以将其分为 1000 个块，每个块包含 10000 个条目："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "\n",
    "NUM_BATCHES = 1000\n",
    "\n",
    "schema = pa.schema([pa.field('nums', pa.int32())])\n",
    "\n",
    "with pa.OSFile('bigfile.arrow', 'wb') as sink:\n",
    "   with pa.ipc.new_file(sink, schema) as writer:\n",
    "      for row in range(NUM_BATCHES):\n",
    "            batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n",
    "            writer.write(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记录批次支持多列，因此实际上我们总是写入相当于一个表的数据。\n",
    "\n",
    "分批写入是有效的，因为理论上我们只需要在内存中保持当前正在写入的批次。但在读取回来时，我们可以更有效，通过直接从磁盘映射数据，避免在读取时分配任何新内存。\n",
    "\n",
    "在正常情况下，读取我们的文件将消耗几百兆字节的内存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN: 10000000\n",
      "RSS: 38MB\n"
     ]
    }
   ],
   "source": [
    "with pa.OSFile('bigfile.arrow', 'rb') as source:\n",
    "   loaded_array = pa.ipc.open_file(source).read_all()\n",
    "\n",
    "\n",
    "print(\"LEN:\", len(loaded_array))\n",
    "\n",
    "print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了更有效地从磁盘读取大数据，我们可以对文件进行内存映射，这样Arrow可以直接引用从磁盘映射的数据，避免需要分配自己的内存。在这种情况下，操作系统将能够懒加载映射的内存，并在压力下将其换出而无需任何写回成本，从而更容易读取比总内存还要大的数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN: 10000000\n",
      "RSS: 0MB\n"
     ]
    }
   ],
   "source": [
    "with pa.memory_map('bigfile.arrow', 'rb') as source:\n",
    "   loaded_array = pa.ipc.open_file(source).read_all()\n",
    "\n",
    "print(\"LEN:\", len(loaded_array))\n",
    "print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "其他高级 API ，如 {func}`~pyarrow.parquet.read_table`，也提供了 `memory_map` 选项。但在这些情况下，内存映射无法帮助减少驻留内存消耗。详情请参阅[读取 Parquet 和内存映射](https://arrow.apache.org/docs/python/parquet.html#parquet-mmap)。\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
